{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## A simple Hidden Markov Model Postagger\n",
    "\n",
    "You need to implement two parts of the HMM postagger.\n",
    "- A HMM model\n",
    "- viterbi decoding\n",
    "\n",
    "Keep in the following things in mind:\n",
    "- probability smoothing when estimating model parameters\n",
    "- (optional) tune hyperparameter on development set\n",
    "\n",
    "You should get an accuracy of more than **67.0** with proper discounting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, you need to implement you parameter estimation part.\n",
    "from collections import Counter\n",
    "from math import log\n",
    "\n",
    "class HMM(object):\n",
    "    def __init__(self, epsilon=1e-5, training_data=None):\n",
    "        self.epsilon = epsilon\n",
    "        if training_data is not None:\n",
    "            self.fit(training_data)\n",
    "\n",
    "\n",
    "    def fit(self, training_data):\n",
    "        '''\n",
    "        Counting the number of unigram, bigram, cooc and wordcount from the training\n",
    "        data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        training_data: list\n",
    "            A list of training data, each element is a tuple with words and postags.\n",
    "        '''\n",
    "        self.unigram = Counter()    # The count of postag unigram, e.g. unigram['NN']=5\n",
    "        self.bigram = Counter()     # The count of postag bigram, e.g. bigram[('PRP', 'VV')]=1\n",
    "        self.cooc = Counter()       # The count of word, postag, e.g. cooc[('I', 'PRP')]=1\n",
    "        self.wordcount = Counter()  # The count of word, e.g. word['I']=1\n",
    "        self.total_words = 0\n",
    "        self.total_tags = 0\n",
    "    \n",
    "        print('building HMM model ...')\n",
    "        for words, tags in training_data:\n",
    "            # Your code here! You need to implement the ngram counting part. Please count\n",
    "            self.total_words += len(words)\n",
    "            self.total_tags += len(tags)\n",
    "            # - unigram\n",
    "            self.unigram.update(tags)\n",
    "            # - bigram\n",
    "            self.bigram.update(zip(tags[:-1], tags[1:]))\n",
    "            # - cooc\n",
    "            self.cooc.update(zip(words, tags))\n",
    "            # - wordcount\n",
    "            self.wordcount.update(words)\n",
    "\n",
    "        print('HMM model is built.')\n",
    "        self.postags = [k for k in self.unigram]\n",
    "        print self.postags\n",
    "\n",
    "            \n",
    "    def emit(self, words, i, tag):\n",
    "        '''\n",
    "        Given a word and a postag, give the log emission probability of P(word|tag)\n",
    "        Please refer the `foundation of statistial natural language processing`, Chapter 10\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        words: list(str)\n",
    "            The list of words\n",
    "        i: int\n",
    "            The ith word\n",
    "        tag: str    \n",
    "            The postag\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        prob: float\n",
    "            The log probability\n",
    "        '''\n",
    "        # Your code here! You need to implement the log emission probability part.\n",
    "        lambda_1 = 0.9999999\n",
    "        lambda_2 = 0.00000005\n",
    "        lambda_3 = 1 - lambda_1 - lambda_2\n",
    "        prob_1 = 1.0 * self.cooc[tuple((words[i], tag))] / self.unigram[tag]\n",
    "        prob_2 = 1.0 * self.wordcount[words[i]] / self.total_words\n",
    "        prob_3 = 1.0 * self.unigram[tag] / self.total_tags\n",
    "        prob = log(lambda_1*prob_1 + lambda_2*prob_2 + lambda_2*prob_3)\n",
    "        return prob\n",
    "    \n",
    "    \n",
    "    def trans(self, tag, tag1):\n",
    "        '''\n",
    "        Given two postags, give the log transition probability of P(tag1|tag)\n",
    "        Please refer the `foundation of statistial natural language processing`, Chapter 10\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tag: str\n",
    "            The previous postag\n",
    "        tag1: str    \n",
    "            The current postag\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        prob: float\n",
    "            The log probability\n",
    "        '''\n",
    "        # Your code here! You need to implement the log transition probability part.\n",
    "        lambda_1 = 0.9999999\n",
    "        lambda_2 = 0.00000005\n",
    "        lambda_3 = 1 - lambda_1 - lambda_2\n",
    "        prob_1 = 1.0 * self.bigram[tuple((tag, tag1))] / self.unigram[tag]\n",
    "        prob_2 = 1.0 * self.unigram[tag] / self.total_tags\n",
    "        prob_3 = 1.0 * self.unigram[tag1] / self.total_tags\n",
    "        prob = log(lambda_1*prob_1 + lambda_2*prob_2 + lambda_2*prob_3)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building HMM model ...\n",
      "HMM model is built.\n",
      "['VV', 'NN', 'PRP']\n",
      "Counter({'NN': 5, 'VV': 3, 'PRP': 1})\n",
      "Counter({('VV', 'NN'): 3, ('NN', 'VV'): 2, ('PRP', 'VV'): 1})\n",
      "Counter({('chase', 'VV'): 3, ('dog', 'NN'): 2, ('cat', 'NN'): 2, ('I', 'PRP'): 1, ('mouse', 'NN'): 1})\n",
      "Counter({'chase': 3, 'cat': 2, 'dog': 2, 'I': 1, 'mouse': 1})\n"
     ]
    }
   ],
   "source": [
    "# The tiny example.\n",
    "training_dataset = [(['dog', 'chase', 'cat'], ['NN', 'VV', 'NN']),\n",
    "                    (['I', 'chase', 'dog'], ['PRP', 'VV', 'NN']),\n",
    "                    (['cat', 'chase', 'mouse'], ['NN', 'VV', 'NN'])\n",
    "                   ]\n",
    "\n",
    "hmm = HMM(training_data=training_dataset)\n",
    "\n",
    "# Testing if the parameter are correctly estimated.\n",
    "assert hmm.unigram['NN'] == 5\n",
    "assert hmm.bigram['VV', 'NN'] == 3\n",
    "assert hmm.bigram['NN', 'VV'] == 2\n",
    "assert hmm.cooc['dog', 'NN'] == 2\n",
    "print hmm.unigram\n",
    "print hmm.bigram\n",
    "print hmm.cooc\n",
    "print hmm.wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We implement the viterbi decoding algorithm.\n",
    "def viterbi(words, hmm):\n",
    "    '''\n",
    "    Viterbi algorihtm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words: list(str)\n",
    "        The list of words\n",
    "    hmm: HMM\n",
    "        The hmm model\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    result: list(str)\n",
    "        The POS-tag for each word.\n",
    "    '''\n",
    "    # unpack the length of words, and number of postags\n",
    "    N, T = len(words), len(hmm.postags)\n",
    "    \n",
    "    # allocate the decode matrix\n",
    "    score = [[-float('inf') for j in range(T)] for i in range(N)]\n",
    "    path = [[-1 for j in range(T)] for i in range(N)]\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "            for j, tag in enumerate(hmm.postags):\n",
    "                score[i][j] = hmm.emit(words, i, tag)\n",
    "        else:\n",
    "            for j, tag in enumerate(hmm.postags):\n",
    "                best, best_t = -1e20, -1\n",
    "                # Your code here, enumerate all the previous tag\n",
    "                for k, tag0 in enumerate(hmm.postags):\n",
    "                    new_score = score[i-1][k]+hmm.trans(tag0, tag)+hmm.emit(words, i, tag)\n",
    "                    if(best < new_score):\n",
    "                        best = new_score\n",
    "                        best_t = k\n",
    "                score[i][j] = best\n",
    "                path[i][j] = best_t\n",
    "\n",
    "    #\n",
    "    best, best_t = -1e20, -1\n",
    "    for j, tag in enumerate(hmm.postags):\n",
    "        if best < score[len(words)- 1][j]:\n",
    "            best = score[len(words)- 1][j]\n",
    "            best_t = j\n",
    "\n",
    "    result = [best_t]\n",
    "    for i in range(len(words)-1, 0, -1):\n",
    "        # Your code here, back trace to recover the full viterbi decode path\n",
    "        best_t = path[i][best_t]\n",
    "        result.append(best_t)\n",
    "    \n",
    "    # convert POStag indexing to POStag str\n",
    "    result = [hmm.postags[t] for t in reversed(result)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NN', 'VV', 'NN']\n",
      "['PRP', 'VV', 'NN']\n"
     ]
    }
   ],
   "source": [
    "# Test with tiny example.\n",
    "testing_dataset = [['dog', 'chase', 'mouse'],\n",
    "                  ['I', 'chase', 'dog']]\n",
    "\n",
    "for testing_data in testing_dataset:\n",
    "    tags = viterbi(testing_data, hmm)\n",
    "    print tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39832 is training sentences.\n",
      "1700 is development sentences.\n",
      "building HMM model ...\n",
      "HMM model is built.\n",
      "['PRP$', 'VBG', 'VBD', 'VBN', ',', \"''\", 'VBP', 'WDT', 'JJ', 'WP', 'VBZ', 'DT', '#', 'RP', '$', 'NN', 'FW', 'POS', '.', 'TO', 'PRP', 'RB', '-LRB-', ':', 'NNS', 'NNP', '``', 'WRB', 'CC', 'LS', 'PDT', 'RBS', 'RBR', 'CD', 'EX', 'IN', 'WP$', 'MD', 'NNPS', '-RRB-', 'JJS', 'JJR', 'SYM', 'VB', 'UH']\n",
      "accuracy=0.946033\n"
     ]
    }
   ],
   "source": [
    "from dataset import read_dataset\n",
    "\n",
    "train_dataset = read_dataset('./penn.train.pos.gz')\n",
    "devel_dataset = read_dataset('./penn.devel.pos.gz')\n",
    "\n",
    "print('%d is training sentences.' % len(train_dataset))\n",
    "print('%d is development sentences.' % len(devel_dataset))\n",
    "\n",
    "hmm.fit(train_dataset)\n",
    "#print hmm.unigram\n",
    "#print hmm.bigram\n",
    "#print hmm.cooc\n",
    "#print hmm.wordcount\n",
    "\n",
    "n_corr, n_total = 0, 0\n",
    "for devel_data_x, devel_data_y in devel_dataset:\n",
    "    pred_y = viterbi(devel_data_x, hmm)\n",
    "\n",
    "    for pred_tag, corr_tag in zip(pred_y, devel_data_y):\n",
    "        if pred_tag == corr_tag:\n",
    "            n_corr += 1\n",
    "        n_total += 1\n",
    "\n",
    "print(\"accuracy=%f\" % (float(n_corr)/ n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NN', 'VBZ', 'DT', 'RB', 'VBN', 'NN', '.']\n",
      "['PRP', 'IN', 'NN', ',', 'CC', 'PRP', 'VBP', 'VBG', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lets play with the HMM postagger\n",
    "print viterbi(['HMM', 'is', 'a', 'widely', 'used', 'model', '.'], hmm)\n",
    "print viterbi(['I', 'like', 'cat', ',', 'but', 'I', 'hate', 'eating', 'fish', '.'], hmm)\n",
    "\n",
    "# and more you example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Work around the test dataset\n",
    "from __future__ import print_function\n",
    "\n",
    "test_dataset = read_dataset('./penn.test.pos.blind.gz')\n",
    "\n",
    "fpo=open('./penn.test.pos.out', 'w')\n",
    "\n",
    "for test_data_x, test_data_y in test_dataset:\n",
    "    pred_y = viterbi(test_data_x, hmm)\n",
    "    print(\" \".join(y for y in pred_y), file=fpo)\n",
    "\n",
    "fpo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
